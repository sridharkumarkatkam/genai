{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YsOfovVdNPBwCDMWHvLfOaNtqXn4qXTs","timestamp":1708087234018}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","## Installing dependencies"],"metadata":{"id":"S41XMbZCJRPM"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1R6HrGnGeVH","executionInfo":{"status":"ok","timestamp":1707991939458,"user_tz":-330,"elapsed":7129,"user":{"displayName":"Vipul Maheshwari","userId":"13637702448104644568"}},"outputId":"69a3c170-9fb6-4870-baf0-ea792d3dc312"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: lancedb in /usr/local/lib/python3.10/dist-packages (0.5.5)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.7)\n","Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.0.20)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.9.0)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n","Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: deprecation in /usr/local/lib/python3.10/dist-packages (from lancedb) (2.1.0)\n","Requirement already satisfied: pylance==0.9.15 in /usr/local/lib/python3.10/dist-packages (from lancedb) (0.9.15)\n","Requirement already satisfied: ratelimiter~=1.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (1.2.0.post0)\n","Requirement already satisfied: retry>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from lancedb) (0.9.2)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (4.66.1)\n","Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from lancedb) (2.6.1)\n","Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (23.2.0)\n","Requirement already satisfied: semver>=3.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (3.0.2)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from lancedb) (5.3.2)\n","Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (6.0.1)\n","Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from lancedb) (8.1.7)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from lancedb) (2.31.0)\n","Requirement already satisfied: overrides>=0.7 in /usr/local/lib/python3.10/dist-packages (from lancedb) (7.7.0)\n","Requirement already satisfied: pyarrow>=12 in /usr/local/lib/python3.10/dist-packages (from pylance==0.9.15->lancedb) (15.0.0)\n","Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from pylance==0.9.15->lancedb) (1.25.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.25)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n","Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.23)\n","Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.87)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n","Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->lancedb) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->lancedb) (2.16.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->lancedb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->lancedb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->lancedb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->lancedb) (2024.2.2)\n","Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry>=0.9.2->lancedb) (4.4.2)\n","Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.10/dist-packages (from retry>=0.9.2->lancedb) (1.11.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"]}],"source":["!pip install lancedb langchain langchain_community prettytable sentence-transformers huggingface-hub bs4 pypdf pandas"]},{"cell_type":"markdown","source":["## Step 1 : Extracting the relevant information\n","\n","\n","To get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be  website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it's original source first."],"metadata":{"id":"CoeHmze8Jkul"}},{"cell_type":"code","source":["import os\n","from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoader\n","\n","# Put the token values inside the double quotes\n","HF_TOKEN = \"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\"\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n","\n","# Loading the web url and data\n","url_loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")\n","documents_loader = DirectoryLoader('/content/drive/MyDrive/notebooks/Projects/rag_application/data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n","\n","# Creating the instances\n","url_docs = url_loader.load()\n","data_docs = documents_loader.load()\n","\n","# Combining all the data that we ingested\n","docs = url_docs + data_docs"],"metadata":{"id":"mxQwf4pLJjxZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2 : Breaking the information into smaller chunks\n","\n","We've all the necessary data for developing our RAG application. Now, it's time to break down this information into smaller chunks. Later, we'll utilize an embedding model to convert these chunks into their respective embeddings. But why it's important?"],"metadata":{"id":"oY52MPWWKUH4"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)\n","chunks = text_splitter.split_documents(docs)"],"metadata":{"id":"ZJX-NOLwKYQM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3 : Creating the embeddings and store them into a vectordatabase\n","\n","There are two primary methods to generate embeddings for our text chunks. The first involves downloading a model, managing preprocessing, and conducting computations independently. Alternatively, we can leverage Hugging Face's model hub, which offers a variety of pre-trained models for various NLP tasks, including embedding generation.\n","\n","\n","Opting for the latter approach allows us to utilize one of Hugging Face's embedding models. With this method, we simply provide our text chunks to the chosen model, saving us from the resource-intensive computations on our local machines. 💀"],"metadata":{"id":"8NmWSIRmKZz6"}},{"cell_type":"code","source":["from langchain_community.embeddings import HuggingFaceEmbeddings\n","\n","embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n","embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})\n","\n","query = \"Hello I want to see the length of the embeddings for this document.\"\n","len(embeddings.embed_documents([query])[0])"],"metadata":{"id":"pg645dbSH32_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707991963951,"user_tz":-330,"elapsed":800,"user":{"displayName":"Vipul Maheshwari","userId":"13637702448104644568"}},"outputId":"502121a9-8dc7-4221-dd67-d33ed0853a0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["384"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["### Storing the embeddings to a vector database"],"metadata":{"id":"_V8Bdjn_LKSe"}},{"cell_type":"code","source":["import lancedb\n","from langchain_community.vectorstores import LanceDB\n","\n","db = lancedb.connect(\"lance_database\")\n","table = db.create_table(\n","    \"rag_sample\",\n","    data=[\n","        {\n","            \"vector\": embeddings.embed_query(\"Hello World\"),\n","            \"text\": \"Hello World\",\n","            \"id\": \"1\",\n","        }\n","    ],\n","    mode=\"overwrite\",\n",")\n","\n","docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)"],"metadata":{"id":"4QLp4LisLIGG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4 : Create a prompt template which will be fed to the LLM\n","\n","Ok now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what's the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. For example, if you are working with \"Mistral 7B instruct\" and you want the optimal results it's recommended to use the following chat template:\n","\n","\n","```python\n","<s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]\n","```"],"metadata":{"id":"cUdKpqIBLe7j"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","template = \"\"\"\n","{query}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)"],"metadata":{"id":"BQb0Oh5BLj-n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Step 5 : Convert the query to it's relevant embedding using same embedding model.\n","\n","Now, let's talk about the query or question we want to ask our RAG application. We can't just pass the query to our model and expect information in return. Instead, we need to pass the query through the same embedding model used for the chunks earlier. Why is this important?\n","\n","Well, by embedding queries, we allow models to compare them efficiently with previously processed chunks of text. This enables tasks like finding similar documents or generating relevant responses."],"metadata":{"id":"cWRR4Et7LrZM"}},{"cell_type":"markdown","source":["## Step 6 : Fetch K number of documents.\n","\n","Now, let's talk about the retriever. Its job is to dive into the vector database and perform a search to find relevant documents. It returns a set number, let's call it \"k\", of these documents, which are ranked based on their contextual relevance to the query or question you asked."],"metadata":{"id":"ZCv5L0-nL2jC"}},{"cell_type":"code","source":["retriever = docsearch.as_retriever(search_kwargs={\"k\": 3})\n","docs = retriever.get_relevant_documents(\"what did you know about Yolo V7?\")"],"metadata":{"id":"vjCko1mrL82o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 7 : Pass the relevant documents to the LLM and get the response.\n","\n","So far, we've asked our retriever to fetch a set number of relevant documents from the database. Now, we need a language model (LLM) to generate a relevant response based on that context."],"metadata":{"id":"22p5wkPMMAXI"}},{"cell_type":"code","source":["from langchain_community.llms import HuggingFaceHub\n","\n","# Model architecture\n","llm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"\n","model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}\n","model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)"],"metadata":{"id":"WheAmghxMBZo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Step 8 : Create a chain for invoking the LLM.\n","\n","We have everything we want for our RAG application, last thing we need to do is to create a chain for invoking the LLM on our query to generate the response.  \n","\n","There are different types of chains for the different types of use cases, if you like your LLM to remember the context of the chat over the time like the ChatGPT , you would need a memory instance which can be shared among multiple conversation pieces, for such cases, there are conversational chains available."],"metadata":{"id":"f3gcAY-7MD9E"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","rag_chain = (\n","    {\"context\": retriever,  \"query\": RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")\n","\n","response = rag_chain.invoke(\"Who killed Jon Snow?\")\n","print(response)"],"metadata":{"id":"htz4xZbEMJ__","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707992027801,"user_tz":-330,"elapsed":796,"user":{"displayName":"Vipul Maheshwari","userId":"13637702448104644568"}},"outputId":"37d00474-78d2-42d3-b2a8-dbbbf69df976"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: \n","Who killed Jon Snow?\n","\n","Computer:\n","Jon Snow was not actually killed, but he did appear to die in the TV series Game of Thrones. However, in the books, the author George R.R. Martin has not revealed how Jon Snow died or who killed him, leaving it open for interpretation. In the TV series, it was revealed that Melisandre resurrected Jon Snow, bringing him back to life. Therefore, no one technically killed Jon Snow in the TV series, but his death was a significant event in the storyline.\n"]}]}]}